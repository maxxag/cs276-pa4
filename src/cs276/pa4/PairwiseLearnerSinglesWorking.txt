package cs276.pa4;

import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;

import weka.classifiers.Classifier;
import weka.classifiers.functions.LibSVM;
import weka.classifiers.functions.LinearRegression;
import weka.core.Attribute;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SelectedTag;
import weka.filters.Filter;
import weka.filters.unsupervised.attribute.Standardize;

/**
 * Implements Pairwise learner that can be used to train SVM
 *
 */
public class PairwiseLearner extends Learner {
	private LibSVM model;
	public PairwiseLearner(boolean isLinearKernel){
		try{
			model = new LibSVM();
		} catch (Exception e){
			e.printStackTrace();
		}

		if(isLinearKernel){
			model.setKernelType(new SelectedTag(LibSVM.KERNELTYPE_LINEAR, LibSVM.TAGS_KERNELTYPE));
		}
	}

	public PairwiseLearner(double C, double gamma, boolean isLinearKernel){
		try{
			model = new LibSVM();
		} catch (Exception e){
			e.printStackTrace();
		}

		model.setCost(C);
		model.setGamma(gamma); // only matter for RBF kernel
		if(isLinearKernel){
			model.setKernelType(new SelectedTag(LibSVM.KERNELTYPE_LINEAR, LibSVM.TAGS_KERNELTYPE));
		}
	}

	@Override
	public Instances extractTrainFeatures(String train_data_file,
			String train_rel_file, Map<String, Double> idfs) {
		/*
		 * @TODO: Your code here:
		 * Get signal file 
		 * Construct output dataset of type Instances
		 * Add new attribute  to store relevance in the train dataset
		 * Populate data
		 */
		Instances dataset = null;

		/* Build attributes list */
		Instances X = null;

		/* Build X and Y matrices */
		ArrayList<Attribute> attributes = new ArrayList<Attribute>();
		attributes.add(new Attribute("url_w"));
		attributes.add(new Attribute("title_w"));
		attributes.add(new Attribute("body_w"));
		attributes.add(new Attribute("header_w"));
		attributes.add(new Attribute("anchor_w"));
		ArrayList<String> labels = new ArrayList<String>();
		labels.add("+1");
		labels.add("-1");
		attributes.add(new Attribute("relevance", labels));
		int pos_inst = 0, neg_inst = 0;
		X = new Instances("train_dataset", attributes, 0);
		/* Set last attribute as target */
		X.setClassIndex(X.numAttributes() - 1);
		int numAttributes = X.numAttributes();

		try {
			Map<Query,List<Document>> data_map = Util.loadTrainData (train_data_file);
			Map<String, Map<String, Double>> relData = Util.loadRelData(train_rel_file);

			Feature feature = new Feature(idfs);

			/* Add data */
			for (Query query : data_map.keySet()){    
				for (int i = 0; i < data_map.get(query).size(); i++) {
					Document doc1 = data_map.get(query).get(i);
					double rel1 = relData.get(query.toString().trim()).get(doc1.url);
					double[] features1 = feature.extractFeatureVector(doc1, query);
					for (int j = i+1; j < data_map.get(query).size(); j++) {
						Document doc2 = data_map.get(query).get(j);
						double rel2 = relData.get(query.toString().trim()).get(doc2.url);
						double[] features2 = feature.extractFeatureVector(doc2, query);
						double[] instance = new double[numAttributes];
						if (rel1 > rel2) {
							if (pos_inst > neg_inst) {
								for (int k = 0; k < features1.length; ++k) {
									instance[k] = features2[k] - features1[k];   //negative
								}
								instance[numAttributes - 1] = 
										X.attribute(numAttributes-1).indexOfValue("-1");
								neg_inst++;
							} else {
								for (int k = 0; k < features1.length; ++k) {
									instance[k] = features1[k] - features2[k];   //positive
								}
								instance[numAttributes - 1] = 
										X.attribute(numAttributes-1).indexOfValue("+1");
								pos_inst++;
							}
						} else if (rel2 > rel1) {
							if (pos_inst > neg_inst) {
								for (int k = 0; k < features1.length; ++k) {
									instance[k] = features1[k] - features2[k];   //negative
								}
								instance[numAttributes - 1] = 
										X.attribute(numAttributes-1).indexOfValue("-1");
								neg_inst++;
							} else {
								for (int k = 0; k < features1.length; ++k) {
									instance[k] = features2[k] - features1[k];   //positive
								}
								instance[numAttributes - 1] = 
										X.attribute(numAttributes-1).indexOfValue("+1");
								pos_inst++;
							}
						}
						Instance inst = new DenseInstance(1.0, instance); 
						X.add(inst);
					}
				}
			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		/* Conduct standardization on X */
		Standardize filter = new Standardize();
		//    Normalize filter = new Normalize(); filter.setScale(2.0); filter.setTranslation(-1.0); // scale values to [-1, 1]
		try {
			filter.setInputFormat(X); 
			dataset = Filter.useFilter(X, filter);
		} catch (Exception e) {
			e.printStackTrace();
		} 

		return dataset;
	}

	@Override
	public Classifier training(Instances dataset) {
		/*
		 * @TODO: Your code here
		 * Build classifer
		 */
		try {
			model.buildClassifier(dataset);
		} catch (Exception e) {
			e.printStackTrace();
		}
		//System.out.println(Arrays.toString(model.coefficients()));
		return model;
	}

	@Override
	public TestFeatures extractTestFeatures(String test_data_file,
			Map<String, Double> idfs) {
		/*
		 * @TODO: Your code here
		 * Use this to build the test features that will be used for testing
		 */
		Instances dataset = null;
		Map<Query, Map<Document, Integer>> index_map = new HashMap<Query, Map<Document, Integer>> ();
		int idx = 0;
		/* Build attributes list */
		Instances X = null;

		/* Build X and Y matrices */
		ArrayList<Attribute> attributes = new ArrayList<Attribute>();
		attributes.add(new Attribute("url_w"));
		attributes.add(new Attribute("title_w"));
		attributes.add(new Attribute("body_w"));
		attributes.add(new Attribute("header_w"));
		attributes.add(new Attribute("anchor_w"));
		ArrayList<String> labels = new ArrayList<String>();
		labels.add("+1");
		labels.add("-1");
		attributes.add(new Attribute("relevance", labels));
		X = new Instances("test_dataset", attributes, 0);
		/* Set last attribute as target */
		X.setClassIndex(X.numAttributes() - 1);
		int numAttributes = X.numAttributes();

		try {
			Map<Query,List<Document>> data_map = Util.loadTrainData (test_data_file);
			Feature feature = new Feature(idfs);

			/* Add data */
			for (Query query : data_map.keySet()){
				index_map.put(query, new HashMap<Document, Integer> ());
				for (Document doc : data_map.get(query)){
					double[] features = feature.extractFeatureVector(doc, query);
					double[] instance = new double[numAttributes];
					for (int i = 0; i < features.length; ++i){
						instance[i] = features[i];
					}
					instance[numAttributes - 1] = 0.0;
					Instance inst = new DenseInstance(1.0, instance); 
					X.add(inst);
					index_map.get(query).put(doc, idx++);
				}
			}
		} catch (Exception e) {
			e.printStackTrace();
		}

		/* Conduct standardization on X */
		Standardize filter = new Standardize();
		//    Normalize filter = new Normalize(); filter.setScale(2.0); filter.setTranslation(-1.0); // scale values to [-1, 1]
		try {
			filter.setInputFormat(X); 
			dataset = Filter.useFilter(X, filter);
		} catch (Exception e) {
			e.printStackTrace();
		}

		return new TestFeatures(dataset, index_map);
	}

	@Override
	public Map<Query, List<Document>> testing(TestFeatures tf,
			Classifier model) {
		Instances dataset = tf.features;
		int numAttributes = dataset.numAttributes();
		Map<Query, List<Document>> result = new HashMap<Query, List<Document>> ();
		Map<Query, Map<Document, Integer>> index_map = tf.index_map;
		try {
			for (Query query : index_map.keySet()) {
				ArrayList<Document> docs = new ArrayList<Document>();
				result.put(query, new ArrayList<Document> ());
				docs.addAll(index_map.get(query).keySet());		
				Collections.sort(docs, new Comparator<Document>() {
					@Override
					public int compare(Document d1, Document d2) {
						Integer idx = null;
						double[] inst1 = {}, inst2 = {};
						idx = index_map.get(query).get(d1);
						if (idx != null) {
							inst1 = dataset.instance(idx).toDoubleArray();
						}
						
						idx = index_map.get(query).get(d2);
						if (idx != null) {
							inst2 = dataset.instance(idx).toDoubleArray();
						}
						
						double[] features = new double[numAttributes];
						for (int i = 0; i < numAttributes; i++) {
							features[i] = inst1[i] - inst2[i];
						}
						Instance classificationInst = new DenseInstance(1.0, features); 
						Instances classifierDataset = new Instances(dataset, 0);
						classifierDataset.add(classificationInst);
						classifierDataset.setClassIndex(numAttributes - 1);
						double prediction = 0;
						String predictStr = "";

						try {
							prediction = model.classifyInstance(classifierDataset.instance(0));
							//System.out.println((int) prediction);
							predictStr = dataset.attribute(numAttributes-1).value((int) prediction);
							//System.out.println("predictStr " + dataset.attribute(numAttributes-1).value((int) prediction) + "maxx");
							//System.out.println(prediction);
						} catch (Exception e) {
							e.printStackTrace();
						}

						if (predictStr.equals("+1")) return -1;
						else if (predictStr.equals("-1")) return 1;
						else {
							//System.out.println("returning default value of prediction");
							return 0;
						}
					}
				});
				result.put(query, docs);
			}
		}
		catch (Exception e) {
			e.printStackTrace();
		}
		return result;
	}

}
